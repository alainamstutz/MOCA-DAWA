---
title: "MOCA/DAWA cluster trial"
author: "A.Amstutz"
date: "2025-02-15"
output:
  html_document:
    keep_md: yes
    toc: yes
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
---
Packages
```{r message=FALSE, warning=FALSE}
RNGkind("L'Ecuyer-CMRG") # simstudy
set.seed(19287) # for reproducibility
library(simstudy)
library(parallel) # for parallelization of core (max 8 on my laptop)

library(lme4)
library(marginaleffects) # marginal standardization
library(insight) # robust SE
library(geepack) # to tackle alternative and more robust (but less efficient?) estimands

library(dplyr)
library(pwr)
library(ggplot2)
library(kableExtra)
```

# Hypothetical MOCA cluster randomized trial (CRT)
Several interventions on the level of health care workers to reduce antibiotic prescriptions at health facilities

Control: Standard of care

Intervention 1: eHealth tool

Intervention 2: eHealth tool + AMR stewardship clubs

Important features and fixed parameters:
- Max. 39 clusters (health centers) due to feasibility/budget
- Binary outcome: Proportion of patients prescribed an antibiotic at first presentation to care
- Baseline prescription rate at control clusters: 75%
- Delta Control to Intervention 1: 25 percentage points, based on previous studies in same setting
- Delta Control to Intervention 2: 30 percentage points, based on previous studies in same setting
- Power min. 80%
- ICC for AB prescription: 0.2, based on previous studies in same setting
- Mean cluster size: 40/month, max. feasible recruitment duration is 5 month => max. mean cluster 200
- There is a variation in cluster size, ratio of standard deviation of cluster sizes to mean of cluster sizes: 0.6-0.8

Design considerations:
- 3-arm vs 2x 2-arm?
- Recruitment bias?
- Secular trend?
- Which pair-wise comparisons to power for?
- Multiplicity?


## First, just as a comparison, a simple individual randomized trial for the same question
```{r, warning=FALSE}
# Parameters
p_C <- 0.75 # control: Baseline prescription rate
p_I1 <- 0.50 # int 1: 25pp reduction
p_I2 <- 0.50 # int 2: 30pp reduction
power <- 0.80 # desired power
alpha <- 0.05 # apply bonferroni correction if adjustment for multiplicity

# Effect sizes
h_I1_C <- ES.h(p1 = p_I1, p2 = p_C)
h_I2_C <- ES.h(p1 = p_I2, p2 = p_C)

cat("Cohen's h for I1 vs Control:", round(h_I1_C, 3), "\n")
cat("Cohen's h for I2 vs Control:", round(h_I2_C, 3), "\n")
# => reduction of mind. 25% is a Cohen's h of over 0.5 -> medium to large effect according to Cohen

# Sample size first pair-wise comparison (I1 vs C)
ss_I1_C <- pwr.2p.test(h = h_I1_C, sig.level = alpha, power = power)
cat("Sample size per arm (I1 vs C):", ceiling(ss_I1_C$n), "\n")

# Sample size second pair-wise comparison (I2 vs C)
ss_I2_C <- pwr.2p.test(h = h_I2_C, sig.level = alpha, power = power)
cat("Sample size per arm (I2 vs C):", ceiling(ss_I2_C$n), "\n")

# Use max of the two
n_per_arm <- max(ceiling(ss_I1_C$n), ceiling(ss_I2_C$n))
n_total <- n_per_arm * 3

cat("Sample size per arm:", n_per_arm, "\n")
cat("Total sample size (3-arm trial):", n_total)
```
A reduction of at least 25% percentage points is a Cohen's h of over 0.5 => medium to large effect.

Adjust for multiplicity yes/no? TBD


## Now, move to a CRT design

(1) Let's figure out the design effect for clustering, to add to individual RCT sample size.

The usual: DEFF = 1 + (m − 1) x ICC, whereby m = cluster size

However, let's not forget the cluster size variation! The usual conservative adjustment of the DEFF with cluster size variation:

DEFF_cv = 1 + (m x (1 + CV^2) - 1) x ICC, whereby CV is the coefficient of variation (ratio of standard deviation of cluster sizes to mean of cluster sizes)
Same as implemented and recommended here: https://pmc.ncbi.nlm.nih.gov/articles/PMC7394950/#sup1

(2) Let's figure out the outcome model:
We have a binary outcome and will use a logistic model to model the log-odds (logit) of success.
So, we have to convert the linear predictor into a probability using the inverse logit (logistic function) and will draw form a Bernoulli distribution:

P(Y_ij = 1) = e_ηij / 1 + e_ηij, whereby ηij = c_j + β x rx_j (the linear predictor for individual i in cluster j)
c_j = the random cluster effect (cluster-specific deviation from the overall average)
β = the regression coefficient,
rx_j = the treatment status of cluster j

After fitting the logistic regression, the inverse logit function is used to convert the log-odds (i.e. ηij) back into a probability.

(3) Let's figure out the ICC:
ICC = Between-site variance / Total variance, whereby the between-site variance represents the clustering. 

In logistic models, the ICC is usually fixed at: π^2 / 3 = 3.29 for the residual level (individual variation).

So, the between-site variance (σ^2c), i.e. cluster-level noise, is what we need, and is therefore derived as:

ICC = σ^2c / σ^2c + (π^2 / 3)

(If there’s additional within-site variation over time, i.e. baseline period, we include σ^2cp, typically as a fraction of σ^2c, e.g., half the site-level variance -> for a later stage).
```{r, warning=FALSE}
# Parameters
p_C <- 0.75 # control: Baseline prescription rate
p_I1 <- 0.50 # int 1: 25pp reduction
p_I2 <- 0.50 # int 2: 30pp reduction
power <- 0.80 # desired power
ICC <- 0.20

m <- 40

alpha_familywise <- 0.05
k <- 2  # number of comparisons
alpha_bonf <- alpha_familywise # no correction
# alpha_bonf <- alpha_familywise / k # Bonferroni corrected alpha

CV <- 0 # no variation
# CV <- 0.1 # 0.1 variation

deff <- 1 + (m-1) * ICC
deff_cv <- 1 + ((m*(1+CV^2))-1) * ICC # with cluster size variation

# Effect sizes
h_I1_C <- ES.h(p1 = p_I1, p2 = p_C)
h_I2_C <- ES.h(p1 = p_I2, p2 = p_C)

# Individual RCT sample sizes for both contrasts
ss1 <- pwr.2p.test(h = h_I1_C, power = 0.80, sig.level = alpha_bonf)$n  # for I1 vs C (individual trial)
ss2 <- pwr.2p.test(h = h_I2_C, power = 0.80, sig.level = alpha_bonf)$n  # for I2 vs C (individual trial)

# CRT sample sizes for both contrasts
ss1_crt <- ss1 * deff_cv
ss2_crt <- ss2 * deff_cv

# Contrast 1 (smaller Delta/Cohens'd => determines overall cluster number)
n_clusters1 <- ceiling(ss1_crt / m)
cat("Cluster sample size int arm 1:", n_clusters1, "\n")
cat("Individual sample size int arm 1:", ss1_crt, "\n")

# Contrast 2
n_clusters2 <- ceiling(ss2_crt / m)
cat("Cluster sample size int arm 2:", n_clusters2, "\n")
cat("Individual sample size int arm 2:", ss2_crt, "\n")

# Total
tot_clusters <- n_clusters1 * 3
tot_ind <- ss1_crt * 3
cat("Total sample size clusters:", tot_clusters, "\n")
cat("Total sample size individuals:", tot_ind, "\n")

```
With CV = 0 => 13 clusters per arm => 39 clusters in total with mean m=40 => ca. 1512 participants 

With CV = 0.6 => 17 clusters per arm => 51 clusters in total with mean m=40 => ca. 2007 participants 

With CV = 0.6 & bonferroni correction => 21 clusters per arm => 63 clusters in total with mean m=40 => ca. 2430 participants

Cluster size increase to m=200 helps little as expected (reduce 2-3 clusters in total)


## Let's generate the data
Using simstudy: https://kgoldfeld.github.io/simstudy/articles/simstudy.html

### Generate a three-arm trial directly
CAVE: It works, but only if I model all outcomes in 1 model (probably not the smartest), hence move to a simpler two-arm trial setup
```{r, warning=FALSE}
# # Define the function to simulate a multi-arm cluster randomized trial
# crt_binary_multiarm_varsize <- function(n_clusters, p0, p1, p2, ICC, cluster_size_mean, cluster_size_sd) {
#   
#   # Convert probabilities to logits
#   logit_p0 <- log(p0 / (1 - p0))  # control
#   logit_p1 <- log(p1 / (1 - p1))  # intervention 1
#   logit_p2 <- log(p2 / (1 - p2))  # intervention 2
# 
#   # Treatment effects (contrasts)
#   beta1 <- logit_p1 - logit_p0  # effect of intervention 1 vs control
#   beta2 <- logit_p2 - logit_p0  # effect of intervention 2 vs control
#   
#   # Step 1: Define the cluster-level treatment assignment (3 arms: control, intervention 1, intervention 2)
#   defC <- defData(varname = "rx", formula = "1;1;1", dist = "trtAssign")
#   
#   # Step 2: Simulate cluster sizes (mean and sd specified by user)
#   cluster_sizes <- rnorm(n_clusters, mean = cluster_size_mean, sd = cluster_size_sd)
#   
#   # Step 3: Define the random effects (ICC) at the cluster level
#   # Between-cluster variance (σ^2c) from ICC
#   sigma2_c <- ICC * (pi^2 / 3) / (1 - ICC)
#   cat("Between-site variance (σ^2c):", sigma2_c, "\n")
#   
#   # Add random effect at cluster level
#   defC <- defData(defC, varname = "c", formula = "0", variance = sigma2_c, dist = "normal")
#   
#   # Step 4: Define individual-level outcome
#   # Step: Add individual-level binary outcome with two treatment contrasts
#   defS <- defDataAdd(varname = "y", 
#                    formula = paste0("c + ", 
#                                     logit_p0, " + ", 
#                                     beta1, " * (rx == 1) + ", 
#                                     beta2, " * (rx == 2)"),
#                    dist = "binary", link = "logit")
#   
#   # Step 5: Generate data for clusters and participants
#   dc <- genData(n_clusters, defC, id = "site")
#   
#   # Step 6: Generate individual-level data (participants per cluster)
#   dd <- genCluster(dc, "site", cluster_sizes, "id")
#   
#   # Step 7: Add individual-level outcome (response) to the data
#   dd <- addColumns(defS, dd)
#   
#   return(dd)
# }
# 
# # Now, let's run the simulation with your parameters
# set.seed(123)  # Set seed for reproducibility
# dd_sim <- crt_binary_multiarm_varsize(
#   n_clusters = 39, 
#   p0 = 0.75, 
#   p1 = 0.50, 
#   p2 = 0.45, 
#   ICC = 0.20, 
#   cluster_size_mean = 40, 
#   cluster_size_sd = 0.8
# )
# 
# # Check the simulated dataset
# table(dd_sim$site)
# table(dd_sim$rx)
# 
# # Check the proportions
# prop_1 <- mean(dd_sim$y[dd_sim$rx == 1])  
# prop_2 <- mean(dd_sim$y[dd_sim$rx == 2])
# prop_3 <- mean(dd_sim$y[dd_sim$rx == 3])
# 
# cat("Observed success rate arm 1:", prop_1, "\n")
# cat("Observed success rate arm 2:", prop_2, "\n")
# cat("Observed success rate arm 3:", prop_3, "\n")
# 
# # Visualize the outcome distribution for each treatment group
# ggplot(dd_sim, aes(x = factor(rx), fill = factor(y))) + 
#   geom_bar(position = "fill", color = "black") + 
#   scale_fill_manual(values = c("gray", "blue"), labels = c("Failure", "Success")) +
#   labs(x = "Treatment Group", y = "Proportion", fill = "Outcome") +
#   theme_minimal() +
#   ggtitle("Proportion of Success by Treatment Group in Multi-Arm Cluster CRT")

```

### Generate a two-arm trial, based on primary and sample size determining contrast
That means max 26 clusters, all other parameters remain the same
```{r, warning=FALSE}
crt_binary_twoarm_varsize <- function(n_clusters, p0, p1, ICC, cluster_size_mean, CV) {
  
  logit_p0 <- log(p0 / (1 - p0))
  logit_p1 <- log(p1 / (1 - p1))
  beta1 <- logit_p1 - logit_p0

  # Treatment variable "rx"
  defC <- defData(varname = "rx", formula = "1;1", dist = "trtAssign")
  
  # Cluster sizes, two options:
  
  # (1) Normal distribution and calculate SD from CV, as per formula
  # cluster_size_sd <- CV * cluster_size_mean
  # cluster_sizes <- round(rnorm(n_clusters, mean = cluster_size_mean, sd = cluster_size_sd)) # ensure no empty nor negative cluster sizes
  
  # (2) Use a gamma distribution to simulate strictly positive, slightly right-skewed cluster sizes (as common in real data, i.e., few large ones)
  shape <- 1 / CV^2
  scale <- cluster_size_mean * CV^2
  cluster_sizes <- round(rgamma(n_clusters, shape = shape, scale = scale))

  # ICC and cluster-level random effect
  sigma2_c <- ICC * (pi^2 / 3) / (1 - ICC)
  cat("Between-site variance (σ^2c):", sigma2_c, "\n")
  defC <- defData(defC, varname = "c", formula = "0", variance = sigma2_c, dist = "normal")

  # Generate the clusters, variable "site"
  dc <- genData(n_clusters, defC, id = "site")

  # Generate the individuals, variable "id"
  dd <- genCluster(dc, "site", cluster_sizes, "id")

  # Add individual-level noise
  dd <- addColumns(defDataAdd(varname = "noise", formula = "0", variance = 1.0, dist = "normal"), dd)

  # Outcome model, based on individual-level outcomes, variable "y" (y = 1/0)
  defS <- defDataAdd(varname = "y", 
                     formula = paste0("c + noise + ", logit_p0, " + ", beta1, " * (rx == 1)"),
                     dist = "binary", link = "logit")
  dd <- addColumns(defS, dd)

  return(dd)
}

# set.seed(342)  # Set seed for reproducibility

dd_sim <- crt_binary_twoarm_varsize(
  n_clusters = 26, 
  p0 = 0.75, 
  p1 = 0.50,
  ICC = 0.20, 
  cluster_size_mean = 40, 
  CV = 0.6
)

## Check the simulated dataset
# table(dd_sim$rx, dd_sim$site)
table(dd_sim$rx)

# Check the proportions
prop_1 <- mean(dd_sim$y[dd_sim$rx == 0])  
prop_2 <- mean(dd_sim$y[dd_sim$rx == 1])

# Visualize the outcome distribution for each treatment group
ggplot(dd_sim, aes(x = factor(rx), fill = factor(y))) + 
  geom_bar(position = "fill", color = "black") + 
  scale_fill_manual(values = c("gray", "blue"), labels = c("No AB prescribed", "AB prescribed")) +
  labs(x = "Treatment Group", y = "Proportion", fill = "Outcome") +
  theme_minimal() +
  ggtitle("Proportion of reaching outcome by treatment group")

# Visualize the clusters and their variability
cluster_summary <- dd_sim %>%
  group_by(site, rx) %>%
  summarise(cluster_size = n(), .groups = "drop")
mean_sizes <- cluster_summary %>%
  group_by(rx) %>%
  summarise(mean_size = mean(cluster_size))
ggplot(cluster_summary, aes(x = factor(site), y = cluster_size, fill = factor(rx))) +
  geom_bar(stat = "identity", color = "black") +
  geom_hline(data = mean_sizes, aes(yintercept = mean_size, color = factor(rx)), 
             linetype = "dashed", size = 1, show.legend = FALSE) +
  geom_text(data = mean_sizes, aes(x = Inf, y = mean_size, label = paste0("Mean = ", round(mean_size, 1))),
            hjust = 1.1, vjust = -0.5, color = c("skyblue4", "tomato3"), size = 4) +
  scale_fill_manual(values = c("skyblue", "tomato"), labels = c("Control (rx=0)", "Intervention (rx=1)")) +
  scale_color_manual(values = c("skyblue4", "tomato3")) +
  labs(x = "Cluster (Site)", y = "Cluster Size", fill = "Treatment Group") +
  theme_minimal() +
  ggtitle("Cluster size per site") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### Let's estimate the effect size
```{r, warning=FALSE}
# Ensure treatment variable is a factor with "control" as reference
dd_sim <- dd_sim %>%
  mutate(rx = factor(rx)) %>%
  mutate(rx = relevel(rx, ref = "0"))

model <- glmer(y ~ rx + (1 | site), data = dd_sim, family = binomial)
# model <- glmmTMB(y ~ rx + (1 | site), data = dd_sim, family = binomial) # Alternative: glmmTMB instead to get RSEs via marginaleffects package, TBD

# Wald
wald <- summary(model)$coefficients
wald_est <- wald["rx1", "Estimate"]
wald_se <- wald["rx1", "Std. Error"]
wald_pval <- wald["rx1", "Pr(>|z|)"]
wald_lower <- wald_est - 1.96 * wald_se
wald_upper <- wald_est + 1.96 * wald_se

# Cluster-robust SEs directly using glmmTMB and marginaleffects, TBD 
# vcov_cr <- get_vcov(model, type = "CR2", cluster = dd_sim$site)
# params <- get_parameters(model)
# robust_est <- params$Estimate[params$Parameter == "rx1"]
# robust_se <- sqrt(diag(vcov_cr))["rx1"]
# robust_pval <- 2 * (1 - pnorm(abs(robust_est / robust_se)))
# robust_lower <- robust_est - 1.96 * robust_se
# robust_upper <- robust_est + 1.96 * robust_se

# Combine
results_table <- tibble(
  Method = c("Wald (model-based)"),
  Estimate = round(c(wald_est), 3),
  OR = round(exp(c(wald_est)), 2),
  CI_Lower = round(exp(c(wald_lower)), 2),
  CI_Upper = round(exp(c(wald_upper)), 2),
  p_value = c(wald_pval)
) %>%
  mutate(
    p_value = ifelse(p_value < 0.001, "<0.001", sprintf("%.3f", p_value))
  )

# Display
results_table %>%
  kable("pipe", col.names = c("Method", "Estimate (log-odds)", "Odds Ratio", "95% CI Lower", "95% CI Upper", "p-value")) %>%
  kable_styling(full_width = FALSE)

```

### Let's confirm the power
```{r, warning=FALSE}
replicate_arm1_vs_control <- function() {
  dat <- crt_binary_twoarm_varsize(
    n_clusters = 26, 
    p0 = 0.75, 
    p1 = 0.50,
    ICC = 0.20, 
    cluster_size_mean = 40, 
    CV = 0.6
  )
  
  dat$rx <- factor(dat$rx)
  dat$rx <- relevel(dat$rx, ref = "0")
  
  model <- glmer(y ~ rx + (1 | site), data = dat, family = binomial(link = "logit"))
  pval <- summary(model)$coefficients["rx1", "Pr(>|z|)"]
  return(pval)
}

# Run 1000 simulations in parallel (I have max 8 cores)
# parallel::detectCores()

set.seed(342)  # Set seed for reproducibility

pvals_arm1 <- mclapply(1:1000, function(x) replicate_arm1_vs_control(), mc.cores = 8)

# Compute power
power_arm1 <- mean(unlist(pvals_arm1) < 0.05) # apply bonferroni correction instead?!
cat("Estimated power:", round(power_arm1, 3), "\n")

# Create histogram
p_values <- unlist(pvals_arm1)  # Convert list to vector
ggplot(data.frame(p_values), aes(x = p_values)) +
  geom_histogram(binwidth = 0.05, fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red", linewidth = 1) + # apply bonferroni correction instead?!
  labs(title = "Distribution of p-values from 1000 simulations",
       x = "p-value",
       y = "Frequency") +
  theme_minimal()

```

### Let's discuss the different CRT estimands
See https://doi.org/10.1093/ije/dyac131
And https://journals.sagepub.com/doi/10.1177/09622802241254197
And https://journals.sagepub.com/doi/10.1177/17407745231186094

What’s the estimand of interest? This depends on the question you're trying to answer:

1. Participant-average treatment effect

Question: “What is the average effect of the intervention on an individual patient?”

Each patient contributes equally.

2. Cluster-average treatment effect

Question: “What is the average effect of the intervention per facility?”

Each facility contributes equally.

See examples in publications: 
(1) "For instance, if hospitals act as the cluster and the outcome relates to individual participants (e.g. a hospital-level intervention aiming to reduce mortality in presenting patients), then the participant-average treatment effect will be of most interest, as this represents the population impact of switching from the control to intervention."
(2)"However, in a trial aiming to reduce unnecessary prescribing of antibiotics, in which doctors act as the cluster and outcomes are measured on each participant they treat, then a cluster-average treatment effect may also be of interest, as this provides the intervention’s effect on the clinician’s prescribing habits."

"Consider a trial comparing a quality improvement (QI) intervention to improve outcomes in patients undergoing emergency laparotomy. This intervention involves local QI leads implementing a hospital-wide improvement programme at each cluster. The primary outcome is overall mortality within 90 days and a secondary outcome is whether a senior surgeon is present in the operating theatre (either performing the surgery or supervising a more junior
surgeon in doing so). This outcome is intended to measure the success of the QI intervention in changing hospital practice.
For the primary outcome, we need to decide whether a participant-average or cluster-average treatment effect is desired (i.e. do we want to know the average mortality reduction across patients or across hospitals?) Here, interest clearly lies in the intervention effect on individual patients
(i.e. how many additional lives can be saved through the QI intervention?). Thus, a participant-average treatment effect is most relevant here.
However, the key secondary outcome (whether a senior surgeon is present) is intended to measure treatment success at the cluster level (i.e. whether the intervention was effective in making hospitals change their practice around emergency laparotomies). Hence, for this outcome, a cluster-average estimand may be the most relevant. We note that for the secondary outcome (whether a senior surgeon is present), both a participant-average and cluster-average treatment effect may be of scientific interest, in which case both could be specified (e.g. with the cluster-average treatment effect designated as the primary). However, including both estimands should only be done if both are indeed of scientific interest."
"In this trial, it is plausible that success in implementing the QI intervention may differ between smaller and larger clusters due to differing resource levels available, resulting in an interaction between treatment effect and cluster size."

In CRTs, treatment effects can be estimated either by implementing an analysis either at the cluster level or the individual level.
A cluster-level analysis involves calculating a summary measure for each cluster (e.g. the mean outcome across participants in that cluster) and then comparing cluster-level summaries. 
In contrast, an individual-level analysis typically involves analysing participant-level outcomes using a regression model that accounts for correlations between participants from the same cluster.

However, we can also reweight a cluster-level analysis to give each participant equal weight to target a participant-average treatment effect. Similarly, we could reweight individual-level analyses to give equal weight to each cluster to target a cluster-average treatment effect. For a cluster-level analysis, this is done by weighting each cluster by the number of participants within that cluster, and for a participant-level analysis, this is done by weighting each individual by the inverse number of participants in that cluster.

Another issue in CRTs is that certain commonly used estimators can be biased when the cluster size is informative. Esp. when using:
Mixed-effects models with a random intercept for cluster (and generalized estimating equations (GEEs) with an exchangeable working correlation structure).
Because they they do not give equal weight to each participant. Instead, clusters are weighted by their inverse-variance, which is a function of both the cluster size and the ICC. 

Solution: IEE = independence estimating equation
Unbiased for the participant-average treatment effect, even if cluster size is informative.
IEEs employ an independence working correlation structure in conjunction with robust standard errors to account for clustering.

IEE can be easily implemented in R by using GEEs with a working independence assumption and robust standard errors or by using a standard regression model estimated by maximum likelihood/least squares with cluster-robust standard errors. 
However, IEEs can be less efficient than mixed-effects models or GEEs with an exchangeable working correlation structure so the latter could be used if there is a strong reason a priori to believe that the cluster size will not be informative.

#### Let's investigate the participant-average treatment effect first (using participant-level data only)
```{r, warning=FALSE}
# (1) mixed-effects, glmer
model_glmer <- glmer(y ~ rx + (1 | site), 
                     data = dd_sim, 
                     family = binomial)

# (2) GEE
model_gee <- geeglm(y ~ rx, id = site, 
                         data = dd_sim, 
                         family = binomial(link = "logit"), 
                         corstr = "exchangeable")

# (3) IEE -> different to all above, robust to informative cluster size (assumes no correlation between observations within a cluster, but with robust standard errors...)
model_iee <- geeglm(y ~ rx, id = site, 
                    data = dd_sim, 
                    family = binomial(link = "logit"), # could also use "log" to directly get RR instead of OR (or "identity" for RD, though convergence can be tricky)
                    corstr = "independence")

# See R code in publication

## Extract estimates from all models and compare across
# Mixed-effects model using Wald
wald <- summary(model_glmer)$coefficients
wald_est <- wald["rx1", "Estimate"]
wald_se <- wald["rx1", "Std. Error"]
wald_pval <- wald["rx1", "Pr(>|z|)"]
wald_lower <- wald_est - 1.96 * wald_se
wald_upper <- wald_est + 1.96 * wald_se

# GEE with an exchangeable working correlation structure 
gee_est <- summary(model_gee)$coefficients["rx1", "Estimate"]
gee_se <- summary(model_gee)$coefficients["rx1", "Std.err"]
gee_pval <- summary(model_gee)$coefficients["rx1", "Pr(>|W|)"]
gee_lower <- gee_est - 1.96 * gee_se
gee_upper <- gee_est + 1.96 * gee_se

# IEE with cluster-robust SEs
iee_est <- summary(model_iee)$coefficients["rx1", "Estimate"]
iee_se <- summary(model_iee)$coefficients["rx1", "Std.err"]
iee_pval <- summary(model_iee)$coefficients["rx1", "Pr(>|W|)"]
iee_lower <- iee_est - 1.96 * iee_se
iee_upper <- iee_est + 1.96 * iee_se

# Combine
results_table <- tibble(
  Method = c("Mixed-effect Wald (model-based)", "GEE with exch. corr.", "IEE with robust SE"),
  Estimate = round(c(wald_est, gee_est, iee_est), 3),
  OR = round(exp(c(wald_est, gee_est, iee_est)), 2),
  CI_Lower = round(exp(c(wald_lower, gee_lower, iee_lower)), 2),
  CI_Upper = round(exp(c(wald_upper, gee_upper, iee_upper)), 2),
  p_value = c(wald_pval, gee_pval, iee_pval)
) %>%
  mutate(
    p_value = ifelse(p_value < 0.001, "<0.001", sprintf("%.3f", p_value))
  )

# Display
results_table %>%
  kable("pipe", col.names = c("Method", "Estimate (log-odds)", "Odds Ratio", "95% CI Lower", "95% CI Upper", "p-value")) %>%
  kable_styling(full_width = FALSE)

```

#### Let's investigate the cluster-average treatment effect (using participant-level data only)
Weighted IEE on participant-level data using robust standard errors, with inverse cluster-size weights equal to 1/n_i to give equal weight to each cluster

Think about simpler cluster ATEs in a second step...
```{r, warning=FALSE}
# Calculate cluster sizes
cluster_sizes <- table(dd_sim$site)
dd_sim$cluster_size <- cluster_sizes[as.character(dd_sim$site)]
dd_sim$inv_cluster_size <- 1 / dd_sim$cluster_size

# Similar to IEE for individual ATE, but re-weight for cluster size, using inverse cluster-size weights
model_cluster_iee <- geeglm(y ~ rx, id = site, 
                    data = dd_sim, 
                    weights = inv_cluster_size,
                    family = binomial(link = "logit"),
                    corstr = "independence")

# See R code in publication

## Extract estimates from all models and compare across
# IEE for cluster level
c_iee_est <- summary(model_cluster_iee)$coefficients["rx1", "Estimate"]
c_iee_se <- summary(model_cluster_iee)$coefficients["rx1", "Std.err"]
c_iee_pval <- summary(model_cluster_iee)$coefficients["rx1", "Pr(>|W|)"]
c_iee_lower <- c_iee_est - 1.96 * c_iee_se
c_iee_upper <- c_iee_est + 1.96 * c_iee_se

# Combine
results_table <- tibble(
  Method = c("Cluster-weighted IEE with robust SE"),
  Estimate = round(c(c_iee_est), 3),
  OR = round(exp(c(c_iee_est)), 2),
  CI_Lower = round(exp(c(c_iee_lower)), 2),
  CI_Upper = round(exp(c(c_iee_upper)), 2),
  p_value = c(c_iee_pval)
) %>%
  mutate(
    p_value = ifelse(p_value < 0.001, "<0.001", sprintf("%.3f", p_value))
  )

# Display
results_table %>%
  kable("pipe", col.names = c("Method", "Estimate (log-odds)", "Odds Ratio", "95% CI Lower", "95% CI Upper", "p-value")) %>%
  kable_styling(full_width = FALSE)

```

=> No difference between cluster ATE and individual ATE suggests no informative cluster size: "If there is no informative cluster size, the participant-average and cluster-average effects will coincide and mixed-effects models target this common treatment effect. However, they can be biased for both the participant- and cluster-average estimand in the presence of informative cluster size."

#### Thompson et al. Cluster randomised trials with a binary outcome and a small number of clusters
Conclusion: We recommend that CRTS with 30 or fewer clusters and a binary outcome use an unweighted cluster-level analysis, or GLMM using REPL. Confidence intervals and p-values for both methods should be calculated based on a t-distribution with the number of degrees of freedom defined as the number of clusters minus cluster-level parameters.

SAS code for GLMM using restricted pseudo-likelihood (REPL) and DF adapted:

proc glimmix data = work.china method = rspl; 
  class countyid;
  model totalmissed_cm10/patients = arm / dist=binomial link=logit solution ddfm = bw;
  random int / subject = countyid

Most likely corresponding R code: Article suggests via glmmPQL (MASS package)!
```{r, warning=FALSE}
library(MASS)
# Fit the GLMM using glmmPQL / The random effect is specified using `random = ~1 | cluster_id`
model_pql <- glmmPQL(
  fixed = y ~ rx, 
  random = ~1 | site, 
  family = binomial(link = "logit"), 
  data = dd_sim
)

# Manual Calculation of Degrees of Freedom, P-value, and CI
n_clusters <- length(unique(dd_sim$site))
n_fixed_params <- length(fixef(model_pql))
df_manual <- n_clusters - n_fixed_params # Number of fixed effects at the cluster level = 1 (treatment) + 1 (intercept) = 2 => "degrees of freedom as clusters minus cluster-level parameters"

treatment_coef <- model_pql$coefficients$fixed["rx1"]
treatment_se <- summary(model_pql)$tTable["rx1", "Std.Error"]
t_stat <- treatment_coef / treatment_se
p_value <- 2 * pt(-abs(t_stat), df = df_manual)

t_critical <- qt(0.975, df = df_manual)
ci_lower_log <- treatment_coef - t_critical * treatment_se
ci_upper_log <- treatment_coef + t_critical * treatment_se

# Combine
results_table <- tibble(
  Method = c("Restricted pseudolikelihood GLMM with DoF clusters minus cluster-level parameters"),
  Estimate = round(c(treatment_coef), 3),
  OR = round(exp(c(treatment_coef)), 2),
  CI_Lower = round(exp(c(ci_lower_log)), 2),
  CI_Upper = round(exp(c(ci_upper_log)), 2),
  p_value = c(p_value)
) %>%
  mutate(
    p_value = ifelse(p_value < 0.001, "<0.001", sprintf("%.3f", p_value))
  )

# Display
results_table %>%
  kable("pipe", col.names = c("Method", "Estimate (log-odds)", "Odds Ratio", "95% CI Lower", "95% CI Upper", "p-value")) %>%
  kable_styling(full_width = FALSE)


# To get something better interpretable, calculate marginal effects -> feed into marginaleffects
library(marginaleffects)
# The function calculates the average risk ratio across the entire dataset.
rr_marginaleffects_pql <- avg_comparisons(
  model_pql,
  variables = "rx",
  type = "response",
  comparison = "ratio"
)

# 4. Display Results
summary(rr_marginaleffects_pql)

```
predicted_lo and predicted_hi: These are the average predicted probabilities for the control (_lo) and treatment (_hi) groups, respectively. You can verify that their ratio is equal to the estimated risk ratio.

0.2869 / 0.5657 ≈ 0.507 (The slight difference is due to the non-linear transformation and how the average is calculated, but they are conceptually related)


## Reproduce Clan command from Stata in R
see: https://pubmed.ncbi.nlm.nih.gov/37850046/ 
Cluster-level analysis, including adjustment and risk ratio output.
However, only 1 stratification variable implemented in Clan command.

Aim: Reproduce Clan in R and allow more stratification variables.
```{r, warning=FALSE}
# This version includes 2 stratification variables `stratum1` and `stratum2`
crt_binary_twoarm_varsize_y0_strat <- function(n_clusters, p0, p1, ICC, cluster_size_mean, CV) {
  
  logit_p0 <- log(p0 / (1 - p0))
  logit_p1 <- log(p1 / (1 - p1))
  beta1 <- logit_p1 - logit_p0

  # Define cluster-level variables including stratification
  defC <- defData(varname = "rx", formula = "1;1", dist = "trtAssign")
  defC <- defData(defC, varname = "stratum1", formula = "0.5", dist = "binary")
  defC <- defData(defC, varname = "stratum2", formula = "1;1;1", dist = "categorical")

  sigma2_c <- ICC * (pi^2 / 3) / (1 - ICC)
  cat("Between-site variance (σ^2c):", sigma2_c, "\n")
  defC <- defData(defC, varname = "c", formula = "0", variance = sigma2_c, dist = "normal")

  dc <- genData(n_clusters, defC, id = "site")
  
  shape <- 1 / CV^2
  scale <- cluster_size_mean * CV^2
  cluster_sizes <- round(rgamma(n_clusters, shape = shape, scale = scale))
  cluster_sizes[cluster_sizes <= 0] <- 1 

  dd <- genCluster(dc, "site", cluster_sizes, "id")

  defY0 <- defDataAdd(varname = "y0", formula = 0.4, dist = "binary") 
  dd <- addColumns(defY0, dd)
  
  # defNoise <- defDataAdd(varname = "noise", formula = "0", variance = 1.0, dist = "normal")
  # dd <- addColumns(defNoise, dd)
  
  defS <- defDataAdd(
    varname = "y1", 
    formula = paste0("c + ", logit_p0, " + ", beta1, " * rx + 0.3 * y0"), # 0.3 correlation y0 -> y1
    dist = "binary", link = "logit"
  )
  
  dd <- addColumns(defS, dd)

  return(dd)
}

# Set seed for reproducibility
set.seed(342) 

# Call the function with parameters for the **success** probabilities
df_crt_b <- crt_binary_twoarm_varsize_y0_strat(
  n_clusters = 26, 
  p0 = 0.75, 
  p1 = 0.50,
  ICC = 0.20, 
  cluster_size_mean = 40, 
  CV = 0.6
)

# STAGE 1: Predict expected outcome from baseline and stratification variables
# The model now includes `y0`, `stratum1`, and `stratum2`
model_stage1 <- glm(y1 ~ y0 + stratum1 + stratum2, data = df_crt_b, family = binomial())
df_crt_b$pred <- predict(model_stage1, type = "response")

# Cluster-level observed and expected proportions
# The aggregation step now groups by site, rx, and the stratification variables
cluster_summary <- df_crt_b %>%
  group_by(site, rx, stratum1, stratum2) %>%
  summarise(
    obs_prop = mean(y1), 
    exp_prop = mean(pred), 
    .groups = "drop"
  ) %>%
  mutate(
    ratio_residual = obs_prop / exp_prop
  )

# STAGE 2: Compare ratio residuals across arms, adjusting for stratification
# The linear model now includes `stratum1` and `stratum2` as predictors
model_stage2 <- lm(log(ratio_residual) ~ rx + stratum1 + stratum2, data = cluster_summary)

# Extract adjusted log risk ratio and 95% CI
log_rr <- coef(model_stage2)["rx"]
se <- summary(model_stage2)$coefficients["rx", "Std. Error"]
df <- df.residual(model_stage2)
t_crit <- qt(0.975, df)

# Confidence interval and final output
log_rr_ci <- c(log_rr - t_crit * se, log_rr + t_crit * se)
rr <- exp(log_rr)
rr_ci <- exp(log_rr_ci)

cat("Adjusted Risk Ratio for SUCCESS (RR_success):", round(rr, 3), "\n")
cat("95% CI:", round(rr_ci[1], 3), "-", round(rr_ci[2], 3), "\n")

# # To get the Risk Ratio for FAILURE, we simply take the reciprocal
# rr_failure <- 1/rr
# rr_failure_ci <- 1/rev(rr_ci)
# 
# cat("\nAdjusted Risk Ratio for FAILURE (RR_failure):", round(rr_failure, 3), "\n")
# cat("95% CI:", round(rr_failure_ci[1], 3), "-", round(rr_failure_ci[2], 3), "\n")

# Plot 1: Raw observed cluster proportions by arm
ggplot(cluster_summary, aes(x = factor(rx), y = obs_prop)) +
  geom_jitter(width = 0.1, height = 0, size = 2, alpha = 0.8) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.3, color = "red") +
  labs(x = "Treatment Arm (rx)", y = "Observed Cluster-Level Proportion",
       title = "Cluster-Level Outcome Proportions by Arm") +
  theme_minimal()

# Plot 2: Adjusted ratio residuals by arm (used for adjusted RR)
ggplot(cluster_summary, aes(x = factor(rx), y = ratio_residual)) +
  geom_jitter(width = 0.1, height = 0, size = 2, alpha = 0.8) +
  stat_summary(fun = mean, geom = "crossbar", width = 0.3, color = "blue") +
  labs(x = "Treatment Arm (rx)", y = "Adjusted ratio residual (Observed / Expected)",
       title = "Adjusted cluster residuals by arm") +
  theme_minimal()
```

